{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"'''\nImportation des bibliothèques nécessaires\n'''\nimport gc\nimport math\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.losses import mean_squared_error as mse_loss\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\nfrom keras import optimizers\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Embedding, concatenate, Flatten\nfrom tensorflow.keras.optimizers import Adam, Adamax, RMSprop\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\nfrom tensorflow.keras.losses import mean_squared_error as mse_loss\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import plot_model\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.dpi'] = 64\nplt.rcParams['figure.figsize'] = [12, 8]\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importation de la Dataset et afficher taille/forme\n\nbuilding = pd.read_csv(\"../input/ashrae-energy-prediction/building_metadata.csv\")\nweather_train = pd.read_csv(\"../input/ashrae-energy-prediction/weather_train.csv\")\nweather_test = pd.read_csv(\"../input/ashrae-energy-prediction/weather_test.csv\")\ntrain = pd.read_csv(\"../input/ashrae-energy-prediction/train.csv\")\ntest = pd.read_csv(\"../input/ashrae-energy-prediction/test.csv\")\n\nprint(\"Size of building data\", building.shape)\nprint(\"Size of weather_train data \", weather_train.shape )\nprint(\"Size of weather_test data\",weather_test.shape )\nprint(\"Size of train data\" , train.shape)\nprint(\"Size of test data\", test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploratory building_metadata.csv\nbuilding.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploratory tweather_rain.csv\nweather_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploratory train.csv\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data processing pour (building_metadata.csv)\ndef process_building_data(building_df):\n    \n    #Ici on a le choix d'effacer les floor_count ou bien de les remplacer par mean()\n    #En effet on a choisit de remplacer par la moyenne \n    \n    print('Data cleaning: remplacement des valeurs nulles pour la colonne floor_count')\n    building_df.floor_count.fillna(round(building_df.floor_count.mean(),0), inplace=True)\n  \n    print('Création d une colonne building_age')\n    current_year = datetime.now().year\n    building_df['building_age'] = current_year - building_df['year_built']\n    \n    print('Data Cleaning: Suppression de la colonne floor_count')\n    building_df.drop(columns=['year_built'], inplace=True)\n\n    print('Data cleaning: Remplacement des valeurs nulles dans la colonne building_age')\n    building_df.building_age.fillna(round(building_df.building_age.mean(),0), inplace=True)\n\n    print('Label encoding primary use')\n    le = LabelEncoder()\n    le_primary_use = le.fit_transform(building.primary_use)\n\n    print('Feature engineering: Normalisation logarithmique pour la colonne square_feet')\n    building_df['log_square_feet'] = np.log(building_df['square_feet'])\n    building_df.drop(columns=['square_feet'], inplace=True)\n\n    building_df['primary_use'] = le_primary_use\n    #On efface maintenain : current_year, le & le_primary_use\n    del current_year, le, le_primary_use\n    gc.collect()\n\n    return building_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data processing forweather dataset\ndef convert_season(month):\n    if (month <= 2) | (month == 12):\n        return 0\n    # 0 = c'est la saison d'Hiver \n    elif month <= 5:\n        return 1\n    # 1 = c'est le printemp\n    elif month <= 8:\n        return 2\n    # 2 = été \n    elif month <= 11:\n        return 3\n    # saison automne\n\ndef convert_direction(degrees):\n    if degrees <= 90:\n        return 0\n    # On est dans la direction  nord-est\n    elif degrees <= 180:\n        return 1\n    # On est dans la direction sud-est\n    elif degrees <= 270:\n        return 2\n    # direction sud-ouest\n    elif degrees <= 360:\n        return 3\n    # direction nord-ouest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creation de l'échelle de beaufort \n#explication \n'''\nL'échelle de Beaufort est une échelle de mesure empirique,\ncomportant 13 degrés (de 0 à 12), de la vitesse moyenne du vent sur une durée \nde dix minutes utilisée dans les milieux maritimes. \n'''\ndef add_beaufort_scale(df):\n  beaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n\n  for item in beaufort:\n    df.loc[(df['wind_speed'] >= item[1]) & (df['wind_speed'] < item[2]), 'beaufort_scale'] = item[0]\n  return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remplacer les données de wheather qui sont null\ndef replace_weather_data_nans(weather_df):\n  weather_df = weather_df.set_index(['site_id','day_of_month','month'])\n                        \n  weather_features_with_nan = {\n      'air_temperature': {'round': 1, 'fill': False},\n      'dew_temperature': {'round': 1, 'fill': False},\n      'wind_speed': {'round': 1, 'fill': False},\n      'wind_direction': {'round': 0, 'fill': False},\n      'sea_level_pressure': {'round': 1, 'fill': True},\n      'precip_depth_1_hr': {'round': 0, 'fill': True},\n      'cloud_coverage': {'round': 0, 'fill': True}\n  }\n\n  for feature, params in weather_features_with_nan.items():\n    print('Data cleaning: Replacing NAN values for ' + feature)\n    filler_df = pd.DataFrame(weather_df.groupby(['site_id','day_of_month','month'])[feature].mean().round(params['round']),\n                             columns=[feature])\n\n    if params['fill']:\n      filler_df_mean = round(filler_df[feature].mean(),params['round'])\n      filler_df.fillna(filler_df_mean, inplace=True)\n    #créer un dataframe de air_temperatures à remplir\n    temporary_df = pd.DataFrame({feature : weather_df[feature]})\n\n    # mettre à jour les valeurs NA de air_temperatures\n    temporary_df.update(filler_df, overwrite=False)\n\n    # mise à jour dans weather dataset\n    weather_df[feature] = temporary_df[feature]\n  weather_df = weather_df.reset_index()\n  return weather_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_weather_data(weather_df):\n  print('Data cleaning: conversion de la colonne timestamp')\n  weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"])\n\n  print('Ajout de colonnes de mois, jour de semaine, jour de mois et heure')\n  weather_df['month'] = weather_df['timestamp'].dt.month\n  weather_df['day_of_week'] = weather_df['timestamp'].dt.dayofweek\n  weather_df['day_of_month']= weather_df['timestamp'].dt.day\n  weather_df['hour'] = weather_df['timestamp'].dt.hour\n\n  print('djout d une colonne  is_weekend')\n  weather_df['is_weekend'] = weather_df.day_of_week.apply(lambda x: 1 if x>=5 else 0)\n\n  print('ajout de la colone  season feature')\n  weather_df['season'] = weather_df.month.apply(convert_season)\n\n  print('data cleaning:remplacement des valeurs manquantes')\n  weather_df = replace_weather_data_nans(weather_df)\n\n  print('ajout de la colonne de Beaufort')\n  weather_df = add_beaufort_scale(weather_df)\n\n  print('ajout de colonne wind_compass_direction ')\n  weather_df['wind_direction'+'_compass'] = weather_df.wind_direction.apply(convert_direction)\n\n  print('ajout colone wind_direction_sin ')\n  weather_df['wind_direction'+'_sin'] = np.sin((2*np.pi*weather_df['wind_direction'])/360)\n\n  print('data cleaning: suppression des la colonne wind_direction')\n  weather_df.drop(columns=['wind_direction'], inplace=True)\n\n  return weather_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#On définie  la fonction de reduction de l'utilisation de la mémoire       \ndef reduce_memory_usage(df):\n  d_types = {'building_id': np.uint16,\n            'site_id': np.uint8,\n            'meter': np.uint8,\n            'primary_use': np.uint8,\n            'log_square_feet': np.float16,\n            'floor_count': np.uint8,\n            'building_age': np.uint8,\n            'air_temperature': np.float16,\n            'dew_temperature': np.float16,\n            'wind_speed': np.float16,\n            'cloud_coverage': np.uint8,\n            'precip_depth_1_hr': np.int16,\n            'sea_level_pressure': np.float32,\n            'wind_direction_sin': np.float16,\n            'wind_direction_compass': np.uint8,\n            'beaufort_scale': np.uint8,\n            'day_of_month': np.uint8,\n            'month': np.uint8,\n            'day_of_week': np.uint8,\n            'hour': np.uint8,\n            'is_weekend': np.uint8,\n            'season': np.uint8\n            }\n\n  for feature in d_types:\n    df[feature] = df[feature].astype(d_types[feature])\n  return df\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_processing_pipeline(df, weather_df, building_df, target_variable='meter_reading'):\n\n  print('Fixer timestamp')\n  df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n  \n  print('\\nTraitement des données building_metadata:\\n')\n  building_df_copy = building_df.copy()\n  building_df_copy = process_building_data(building_df_copy)\n\n  print('\\nTraitement des données weather data:\\n')\n  weather_df_copy = weather_df.copy()\n  weather_df_copy = process_weather_data(weather_df_copy)\n\n  print('\\nFusion des  datasets:\\n')\n  print('Fusion avec building dataset')\n  df = df.merge(building_df_copy, on='building_id', how='left')\n  print('Fusion avec weather dataset')\n  df = df.merge(weather_df_copy, on=['site_id', 'timestamp'], how='left')\n\n  print('\\nData cleaning: Attribuer les donnéé manquantes aprés fusion à  de lensemble')\n  columns_with_nan = df.columns[df.isna().any()]\n  for feature in columns_with_nan:\n    df[feature] = df[feature].fillna(method='ffill')\n\n  print('\\nréduction de l utilisation de mémoire:\\n')\n  start_mem = df.memory_usage().sum() / 1024**2\n  print('L utilisation de la mémoire de la dataframe est {:.2f} MB'.format(start_mem))\n  df = reduce_memory_usage(df)\n  end_mem = df.memory_usage().sum() / 1024**2\n  print('utilisation de la mémoire aprés optimization is: {:.2f} MB'.format(end_mem))\n  print('Donc l utilisation de la mémoire a été diminué de  {:.1f}%\\n'.format(100 * (start_mem - end_mem) / start_mem))\n\n  del weather_df_copy, building_df_copy, start_mem, end_mem\n  gc.collect()\n  return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n%time\n\ntrain_copy = data_processing_pipeline(train, weather_train , building)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First Model creation : (Deep Neural Network)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Definisons d'abord l'erreur \ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(dense_layer_sizes, opt=\"adam\", dropout=0.1, nbr_features=20, dense_nparams=256, lr=0.01):\n    optimizers = {\n        'adam': Adam(lr),\n        'rmsprop': RMSprop(lr)\n    }\n    model = Sequential()\n    model.add(Dense(dense_nparams, activation='relu', input_shape=(nbr_features,))) \n    model.add(BatchNormalization())\n    model.add(Dropout(dropout))\n    for layer_size in dense_layer_sizes:\n        model.add(Dense(layer_size, activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dropout(dropout))\n    model.add(Dense(1, activation='relu'))\n    model.compile(loss='mse', optimizer=optimizers[opt], metrics=[root_mean_squared_error])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(dense_layer_sizes=[256, 128, 64, 32], dense_nparams=512, dropout=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 50\nBATCH_SIZE = 1024\ndef step_decay(epoch):\n   initial_lrate = 0.01\n   drop = 0.9\n   lrate = initial_lrate * math.pow(drop, math.floor((epoch)))\n   return lrate\n\nbest_model_file = \"epochs:{epoch:03d}-val_loss:{val_loss:.3f}.h5\"\n\nmc = ModelCheckpoint(best_model_file, monitor='val_loss', mode='auto',verbose=True, save_best_only=True)\nes = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=True, mode='auto', restore_best_weights=True)\nrlr = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, verbose=True, min_delta=0.001, min_lr=1e-5)\nlrs = LearningRateScheduler(step_decay, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train-test split\nunnecessary_features = ['timestamp', 'day_of_month']\n\ncategoricals = ['building_id', 'site_id', 'meter', 'primary_use', 'floor_count',\n                'cloud_coverage', 'beaufort_scale', 'wind_direction_compass',\n                'day_of_week', 'hour', 'is_weekend', 'season']\n\nnumericals = [\"building_age\", \"log_square_feet\", \"air_temperature\", \"dew_temperature\",\n              \"precip_depth_1_hr\", \"sea_level_pressure\", \"wind_speed\", \"wind_direction_sin\"]\n\ntarget = \"meter_reading\"\n\nfeatures = categoricals + numericals\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRAIN_SIZE = 1047552\n# train_sample = train.sample(TRAIN_SIZE)\n\nX = train_copy[features]\ny = train_copy[target].map(np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\nhist = model.fit(X_train.values, y_train.values, epochs=EPOCHS, batch_size=BATCH_SIZE, \n                 validation_data=(X_test.values, y_test.values), verbose=True, callbacks=[es, mc, rlr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Second method: KFold Cross validation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSecond method: KFold Cross validation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"keras_estimator = KerasRegressor(create_model, epochs=20, batch_size=1024, verbose=2)\nkeras_estimator.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndense_size_candidates = [[64, 32], [128, 64, 32], [256, 128, 64, 32], \n                         [256, 128, 64, 64, 32, 16], [512, 256, 128, 128, 64, 32]]\nparam_grid = {\n    'dense_nparams': [128, 256, 384, 512, 640, 768, 896, 1024],\n    'dense_layer_sizes': dense_size_candidates,\n    'opt':['rmsprop', 'adam'],\n    'lr': [0.001, 0.002, 0.05, 0.01],\n    'dropout': [0.5, 0.3, 0.2, 0.1, 0]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold_splits = 4\ntuner = RandomizedSearchCV(estimator=keras_estimator,\n                    verbose=True,\n                    n_iter=25,\n                    n_jobs=4,\n                    return_train_score=True,\n                    cv=kfold_splits,\n                    param_distributions=param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nkfold_splits = 4\ntuner = RandomizedSearchCV(estimator=keras_estimator,\n                    verbose=True,\n                    n_iter=25,\n                    n_jobs=4,\n                    return_train_score=True,\n                    cv=kfold_splits,\n                    param_distributions=param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" es = EarlyStopping(monitor='loss', min_delta=0.01, patience=5, verbose=True, mode='auto', restore_best_weights=True)\n rlr = ReduceLROnPlateau(monitor='loss', patience=2, factor=0.75, verbose=True, min_delta=0.01, min_lr=1e-5)\n\n tuner_result = tuner.fit(X_train, y_train, callbacks=[es, rlr])\n\n # Affichage du résultats.\n print(\"Best: %f using %s\" % (tuner_result.best_score_, tuner_result.best_params_))\n means = tuner_result.cv_results_['mean_test_score']\n stds = tuner_result.cv_results_['std_test_score']\n params = tuner_result.cv_results_['params']\n for mean, stdev, param in zip(means, stds, params):\n     print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Third method LSTM ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_copy = data_processing_pipeline(test, weather_test, building)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_copy[features]\ny_pred = best_model.predict(test_data.values, batch_size=1024, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Data processing pipeline on test dataset\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creation OF sample_submission.csv \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['meter_reading'] = np.expm1(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"export_csv = sample_submission.to_csv('./my-submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}